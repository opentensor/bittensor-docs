

<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bittensor Architecture &mdash; Bittensor 1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bittensor Component Breakdown" href="component-breakdown.html" />
    <link rel="prev" title="gRPC Protocol" href="grpc-protocol.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Bittensor
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/run-bittensor-via-docker.html">Run Bittensor via Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/run-bittensor-via-python.html">Run Bittensor via Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/run-multiple-bittensor-instances.html">Running Multiple Bittensor Instances</a></li>
</ul>
<p class="caption"><span class="caption-text">Bittensor</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="grpc-protocol.html">gRPC Protocol</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bittensor Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#biological-inspiration">Biological Inspiration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-bittensor-network">The Bittensor Network</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="component-breakdown.html">Bittensor Component Breakdown</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Bittensor</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Bittensor Architecture</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/crate/crate-docs-theme/blob/master/docs/bittensor-deep-dive/bittensor-architecture.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="bittensor-architecture">
<h1>Bittensor Architecture<a class="headerlink" href="#bittensor-architecture" title="Permalink to this headline">¶</a></h1>
<p>The inspiration for Bittensor came from brain cells (called neurons). In the same way that a network
of neurons consists of multiple neurons communicating with each other continuously, so does the Bittensor
network consist of multiple machine learning models (also called neurons) communicating with each other.</p>
<p>Therefore to help in understanding the Bittensor network architecture, we use
biological neurons as inspiration and an analogous example.</p>
<div class="section" id="biological-inspiration">
<h2>Biological Inspiration<a class="headerlink" href="#biological-inspiration" title="Permalink to this headline">¶</a></h2>
<p>The Figure below describes a biological neuron that contains three important parts:</p>
<ol class="arabic simple">
<li><strong>Dendrite</strong>: responsible for receiving information from other neurons.</li>
<li><strong>Axon Terminal</strong>: responsible for sending information to other neurons.</li>
<li><strong>Soma</strong>: Contains the nucleus and other structures common to living cells.</li>
</ol>
<p>These three parts together enable a neuron to exchange messages with each other and form a
massive interconnected network that – on the grand scale – form a brain. Note that
this is a highly simplified explanation, and biological neurons are significantly more complex.</p>
<div class="figure" id="id1">
<img alt="../_images/bio_neuron.jpg" src="../_images/bio_neuron.jpg" />
<p class="caption"><span class="caption-text">Biological neuron cell. Credit: David Baillot/ UC San Diego.</span></p>
</div>
</div>
<div class="section" id="the-bittensor-network">
<h2>The Bittensor Network<a class="headerlink" href="#the-bittensor-network" title="Permalink to this headline">¶</a></h2>
<p>Similarly to its biological counterpart, the Bittensor neuron also exchanges messages with other
neurons in the form of <cite>machine knowledge</cite>. A group of these neurons can connect together to form a
network of interconnected models that are able to learn from each other.</p>
<p>Bittensor Neuron examples can be found under <code class="code docutils literal notranslate"><span class="pre">examples</span></code>. Presently, there are 3 examples:</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/opentensor/bittensor/tree/master/examples/mnist">MNIST</a> : The most basic neuron example. A simple feed-forward network that takes 32x32 images as input.</li>
<li><a class="reference external" href="https://github.com/opentensor/bittensor/tree/master/examples/cifar">CIFAR</a> : A more complicated Dual Path Neural Network (As described by <a class="reference external" href="https://arxiv.org/abs/1707.01629">Chen et al.</a>) that takes as input the CIFAR-10 dataset.</li>
<li><a class="reference external" href="https://github.com/opentensor/bittensor/tree/master/examples/bert">BERT</a> : A complete implementation of <a class="reference external" href="https://huggingface.co/transformers/">Huggingface’s bert model</a>.</li>
</ul>
<p>For the rest of this document, we will refer to <strong>MNIST</strong> during our examples as it is the most straightforward network, and hence
easier to understand the architecture of Bittensor through it.</p>
<p>Assume we have two neurons as in the figure below, where each neuron contains 3 main components:</p>
<ol class="arabic simple">
<li><strong>Axon Terminal</strong>: Responsible for deploying a synapse and receiving information coming from a remote synapse.</li>
<li><strong>Dendrite</strong>: Responsible for sending information to a remote synapse.</li>
<li><strong>Neuron</strong>: Effectively the “soma” of a bittensor node. Contains the local model as well as the training and testing logic.</li>
</ol>
<div class="figure" id="id2">
<img alt="../_images/Bittensor.png" src="../_images/Bittensor.png" />
<p class="caption"><span class="caption-text">Communication between two Bittensor neurons.</span></p>
</div>
<p>Let’s also assume that as per the <a class="reference internal" href="../getting-started/run-multiple-bittensor-instances.html"><span class="doc">getting started</span></a> guide, we started Neuron 1 first.</p>
<p>Let’s go through a breakdown of what happens when we start a Neuron. You can follow along in the <a class="reference external" href="https://github.com/opentensor/bittensor/blob/master/examples/mnist/main.py#L198">MNIST code</a>.</p>
<ol class="arabic">
<li><p class="first">Neuron loads its configuration (set by the command line argumnets) and dataset, then sets up the hyper parameters such as the learning rate, batch size, etc.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Additional training params.</span>
<span class="n">batch_size_train</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batch_size_test</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="o">...</span>
<span class="c1"># Load (Train, Test) datasets into memory.</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">model_toolbox</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_train</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">model_toolbox</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_test</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Neuron builds the local synapse (this is the model to be trained on the network).</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build local synapse to serve on the network.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MnistSynapse</span><span class="p">()</span> <span class="c1"># Synapses take a config object.</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span> <span class="n">device</span> <span class="p">)</span> <span class="c1"># Send model to device (GPU or CPU)</span>
</pre></div>
</div>
</div></blockquote>
<p>where <code class="code docutils literal notranslate"><span class="pre">MnistSynapse</span></code> is a class that extends the <code class="code docutils literal notranslate"><span class="pre">Synapse</span></code> class.
In true Pytorch fasion, it contains an <code class="code docutils literal notranslate"><span class="pre">__init__</span></code> and a <code class="code docutils literal notranslate"><span class="pre">forward()</span></code> call.</p>
</li>
<li><p class="first">The Neuron will then build and start the <code class="code docutils literal notranslate"><span class="pre">Metagraph</span></code> object.
This object is responsible for connecting to the Blockchain
and finding other neurons on the network.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">metagraph</span> <span class="o">=</span> <span class="n">bittensor</span><span class="o">.</span><span class="n">Metagraph</span><span class="p">(</span> <span class="n">config</span> <span class="p">)</span>
<span class="n">metagraph</span><span class="o">.</span><span class="n">subscribe</span><span class="p">(</span> <span class="n">model</span> <span class="p">)</span> <span class="c1"># Adds the synapse to the metagraph.</span>
<span class="n">metagraph</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># Starts the metagraph gossip threads.</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">The Axon server is built next. This server allows other neurons
(Neuron 2 in this example) to make queries to Neuron 1 through
the Dendrite. It also deploys the synapse model we set up in step 2.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">axon</span> <span class="o">=</span> <span class="n">bittensor</span><span class="o">.</span><span class="n">Axon</span><span class="p">(</span> <span class="n">config</span> <span class="p">)</span>
<span class="n">axon</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="p">)</span>
<span class="n">axon</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># Starts the server background threads. Must be paired with axon.stop().</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Neuron builds the Dendrite object next. The Dendrite is
responsible for sending dataset batches across the
network to remote synapses.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dendrite</span> <span class="o">=</span> <span class="n">bittensor</span><span class="o">.</span><span class="n">Dendrite</span><span class="p">(</span> <span class="n">config</span> <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Finally, the Neuron builds the router. The router is
responsible for learning <strong>which</strong> synapse to call.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">router</span> <span class="o">=</span> <span class="n">bittensor</span><span class="o">.</span><span class="n">Router</span><span class="p">(</span><span class="n">x_dim</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">key_dim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">topk</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">We can set up the optimizer the same way we normally do
with any other Pytorch model. The important piece here is that
we are optimizing both the model parameters <strong>and</strong> the
router’s parameters, as we are dealing with two models here.
The Synapse model that we are training, and the router’s
model that learns which synapse model to tell the Dendrite
to send a dataset batch to.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the optimizer.</span>
<span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">router</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">If we have previously saved a Bittensor model
and wish to continue training it, we can load it
back up using the <code class="code docutils literal notranslate"><span class="pre">model_toolbox</span></code>.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load previously trained model if it exists</span>
<span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_hparams</span><span class="o">.</span><span class="n">load_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">best_test_loss</span> <span class="o">=</span> <span class="n">model_toolbox</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">_hparams</span><span class="o">.</span><span class="n">load_model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">The training loop proceeds as it would with static
training models that we know and love in Pytorch.
However there is one difference now that they are
communicating with each other: we need to actually tell
them to talk to each other during training. Hence,
during the training loop we invoke the following lines:</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Encode inputs for the router context.</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that by <code class="code docutils literal notranslate"><span class="pre">context</span></code> here we mean the input coming from the local network, which is a vector output of its forward pass. This same context is used as input for the remote networks.</p>
</div></blockquote>
<p>This invokes the model for one forward pass of the image inputs through the
<code class="code docutils literal notranslate"><span class="pre">MnistSynapse</span></code> model we defined. We then query the network
of peers and send them the vector output of this forward pass and the current batch of examples that we are training on.</p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Query the remote network of peers</span>
<span class="n">synapses</span> <span class="o">=</span> <span class="n">metagraph</span><span class="o">.</span><span class="n">get_synapses</span><span class="p">(</span> <span class="mi">1000</span> <span class="p">)</span> <span class="c1"># Returns a list of synapses on the network (max 1000).</span>
<span class="n">requests</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">router</span><span class="o">.</span><span class="n">route</span><span class="p">(</span> <span class="n">synapses</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">images</span> <span class="p">)</span> <span class="c1"># routes inputs to network.</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">dendrite</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span> <span class="n">synapses</span><span class="p">,</span> <span class="n">requests</span> <span class="p">)</span> <span class="c1"># Makes network calls.</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">router</span><span class="o">.</span><span class="n">join</span><span class="p">(</span> <span class="n">responses</span> <span class="p">)</span> <span class="c1"># Joins responses based on scores..</span>
</pre></div>
</div>
</div></blockquote>
<dl class="docutils">
<dt>Let’s unpack this line by line:</dt>
<dd><ul class="first last simple">
<li><code class="code docutils literal notranslate"><span class="pre">metagraph.get_synapses()</span></code> will simply query the network, and return a list of synapses that are presently on the network that we can query. Recall that a remote synapse is a model running on a remote neuron.</li>
<li><code class="code docutils literal notranslate"><span class="pre">router.route()</span></code> will utilize the router model to find which synapses to query that will return the best responses. It will return <code class="code docutils literal notranslate"><span class="pre">requests</span></code>: a list of input minibatches to be sent to the remote synapses, and <code class="code docutils literal notranslate"><span class="pre">scores</span></code>: a list of scores of the performance of the remote synapses.</li>
<li><code class="code docutils literal notranslate"><span class="pre">dendrite.forward_image()</span></code> will forward the minibatches to the remote synapses, and return their responses – a vector output of their <code class="code docutils literal notranslate"><span class="pre">forward</span></code> call.</li>
<li><code class="code docutils literal notranslate"><span class="pre">router.join()</span></code> will join the responses of all the synapses together.</li>
</ul>
</dd>
</dl>
<p><strong>NOTE</strong>: If we are running only one instance of Bittensor with no peers, then this call would simply recursively send the batch and vector output of the forward pass to the instance itself, effectively learning locally as if it’s a local learning model.</p>
</li>
<li><p class="first">Now that we have the responses from all the remote synapses, we can call <code class="code docutils literal notranslate"><span class="pre">forward()</span></code> on the local model to calculate the following:</p>
<ul class="simple">
<li><code class="code docutils literal notranslate"><span class="pre">loss</span></code>: Total loss acumulation to be used by <code class="code docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</li>
<li><code class="code docutils literal notranslate"><span class="pre">local_output</span></code>: Output encoding of image inputs produced by using the local distillation model as context rather than the network.</li>
<li><code class="code docutils literal notranslate"><span class="pre">local_target</span></code>: MNIST Target predictions using student model as context.</li>
<li><code class="code docutils literal notranslate"><span class="pre">local_target_loss</span></code>: MNIST Classification loss computed using the local_output, student model and passed labels.</li>
<li><code class="code docutils literal notranslate"><span class="pre">network_target</span></code>: MNIST Target predictions using the network as context.</li>
<li><code class="code docutils literal notranslate"><span class="pre">network_output</span></code>: Output encoding of inputs produced by using the network inputs as context to the model rather than the student.</li>
<li><code class="code docutils literal notranslate"><span class="pre">network_target_loss</span></code>: MNIST Classification loss computed using the local_output and passed labels.</li>
<li><code class="code docutils literal notranslate"><span class="pre">distillation_loss</span></code>: Distillation loss produced by the student with respect to the network context.</li>
</ul>
<p>Remember that by <code class="code docutils literal notranslate"><span class="pre">context</span></code> here we mean the input coming from the remote network of neurons, which is a vector output of their own forward passes. The student distillation model learns to emulate this context from the network as well to increase accuracy and reduce loss.</p>
</li>
<li><p class="first">Finally, let’s set the weights of the remote synapses. We first get the weights for list of Synapse endpoints, normalize them by the scores we calculated for the remote synapses, then we set them back.
This process is what allows a Bittensor neuron pick the best remote synapses (or peers) upon the next training iteration.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">metagraph</span><span class="o">.</span><span class="n">getweights</span><span class="p">(</span><span class="n">synapses</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.99</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">metagraph</span><span class="o">.</span><span class="n">setweights</span><span class="p">(</span><span class="n">synapses</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="component-breakdown.html" class="btn btn-neutral float-right" title="Bittensor Component Breakdown" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="grpc-protocol.html" class="btn btn-neutral float-left" title="gRPC Protocol" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Ala Shaabana, Jacob Steeves, and the Bittensor community

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>